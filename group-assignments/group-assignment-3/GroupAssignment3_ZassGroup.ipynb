{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Group Assignment 3 (20%)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Group Members\n",
    "\n",
    "-   Ezlan Zulfiqree bin Hashim (17192056)\n",
    "-   Nurulnadiah binti Sukerman (S2178467)\n",
    "-   Nur Dhania Kamalia Kamarol (S2121964)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import the necessary libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the necessary variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64  # Specify the batch size and create data loaders\n",
    "input_size = 16  # The number of features in your dataset\n",
    "hidden1_size = 10  # Number of units in the first hidden layer\n",
    "hidden2_size = 5  # Number of units in the second hidden layer\n",
    "output_size = (\n",
    "    2  # Output size for softmax (assuming binary classification, change if needed)\n",
    ")\n",
    "learning_rate = 0.025  # Stochastic Gradient Descent optimizer learning rate\n",
    "num_epochs = 100  # You can adjust the number of epochs as needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instantiate fetus training and testing datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed for reproducibility\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "\n",
    "# Define a custom dataset class for the fetus dataset\n",
    "class FetusDataset(Dataset):\n",
    "    def __init__(self, csv_file):\n",
    "        le = preprocessing.LabelEncoder()\n",
    "        csv_data = pd.read_csv(csv_file)\n",
    "        csv_features = csv_data.iloc[:, 1:].values\n",
    "        csv_labels = csv_data.iloc[:, 0].values\n",
    "        csv_labels = le.fit_transform(csv_labels)\n",
    "        self.features = torch.tensor(csv_features)\n",
    "        self.labels = torch.tensor(csv_labels)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.features[idx], self.labels[idx]\n",
    "\n",
    "\n",
    "# Define the paths to the training and testing CSV files\n",
    "# train_csv_file = \"Fetus_trainingdata.csv\"\n",
    "# test_csv_file = \"Fetus_testingdata.csv\"\n",
    "\n",
    "# Use the scaled version of the training and testing CSV files for the modified MLP model\n",
    "train_csv_file = \"Normalize_train.csv\"\n",
    "test_csv_file = \"Normalize_test.csv\"\n",
    "\n",
    "# Create training and testing datasets\n",
    "train_dataset = FetusDataset(train_csv_file)\n",
    "test_dataset = FetusDataset(test_csv_file)\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "# You can now use train_loader and test_loader to iterate through your datasets in batches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instantiate MLP class with init and forward methods\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Default Setting: Activation function (HL1: ReLU / HL2: Softmax)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden1_size, hidden2_size, output_size):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden1_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden1_size, hidden2_size)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.softmax(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default\n",
    "\n",
    "\n",
    "# class MLP(nn.Module):\n",
    "#     def __init__(self, input_size, hidden1_size, hidden2_size, output_size):\n",
    "#         super(MLP, self).__init__()\n",
    "#         self.fc1 = nn.Linear(input_size, hidden1_size)\n",
    "#         self.relu_i_h1 = nn.ReLU()\n",
    "#         self.fc2 = nn.Linear(hidden1_size, hidden2_size)\n",
    "#         self.softmax_h1_h2 = nn.Softmax(dim=1)\n",
    "#         self.fc3 = nn.Linear(hidden2_size, output_size)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.fc1(x)\n",
    "#         x = self.relu_i_h1(x)\n",
    "#         x = self.fc2(x)\n",
    "#         x = self.softmax_h1_h2(x)\n",
    "#         x = self.fc3(x)\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### One Hidden Layer Only. Activation function (HL1: ReLU)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One Hidden Layer Only. Activation function (HL1: ReLU)\n",
    "\n",
    "\n",
    "# class MLP(nn.Module):\n",
    "#     def __init__(self, input_size, hidden1_size, hidden2_size, output_size):\n",
    "#         super(MLP, self).__init__()\n",
    "#         self.fc1 = nn.Linear(input_size, hidden1_size)\n",
    "#         self.relu_i_h1 = nn.ReLU()\n",
    "#         self.fc3 = nn.Linear(hidden1_size, output_size)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.fc1(x)\n",
    "#         x = self.relu_i_h1(x)\n",
    "#         x = self.fc3(x)\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Activation function (HL1: ReLU / HL2: ReLU)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activation function (HL1: ReLU / HL2: ReLU)\n",
    "\n",
    "\n",
    "# class MLP(nn.Module):\n",
    "#     def __init__(self, input_size, hidden1_size, hidden2_size, output_size):\n",
    "#         super(MLP, self).__init__()\n",
    "#         self.fc1 = nn.Linear(input_size, hidden1_size)\n",
    "#         self.relu_i_h1 = nn.ReLU()\n",
    "#         self.fc2 = nn.Linear(hidden1_size, hidden2_size)\n",
    "#         self.relu_h1_h2 = nn.ReLU()\n",
    "#         self.fc3 = nn.Linear(hidden2_size, output_size)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.fc1(x)\n",
    "#         x = self.relu_i_h1(x)\n",
    "#         x = self.fc2(x)\n",
    "#         x = self.relu_h1_h2(x)\n",
    "#         x = self.fc3(x)\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Activation function (HL1: Softmax / HL2: Softmax)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activation function (HL1: Softmax / HL2: Softmax)\n",
    "\n",
    "\n",
    "# class MLP(nn.Module):\n",
    "#     def __init__(self, input_size, hidden1_size, hidden2_size, output_size):\n",
    "#         super(MLP, self).__init__()\n",
    "#         self.fc1 = nn.Linear(input_size, hidden1_size)\n",
    "#         self.Softmax_i_h1 = nn.Softmax(dim=1)\n",
    "#         self.fc2 = nn.Linear(hidden1_size, hidden2_size)\n",
    "#         self.softmax_h1_h2 = nn.Softmax(dim=1)\n",
    "#         self.fc3 = nn.Linear(hidden2_size, output_size)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.fc1(x)\n",
    "#         x = self.Softmax_i_h1(x)\n",
    "#         x = self.fc2(x)\n",
    "#         x = self.softmax_h1_h2(x)\n",
    "#         x = self.fc3(x)\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Activation function (HL1: Softmax / HL2: ReLU)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activation function (HL1: Softmax / HL2: ReLU)\n",
    "\n",
    "\n",
    "# class MLP(nn.Module):\n",
    "#     def __init__(self, input_size, hidden1_size, hidden2_size, output_size):\n",
    "#         super(MLP, self).__init__()\n",
    "#         self.fc1 = nn.Linear(input_size, hidden1_size)\n",
    "#         self.Softmax_i_h1 = nn.Softmax(dim=1)\n",
    "#         self.fc2 = nn.Linear(hidden1_size, hidden2_size)\n",
    "#         self.relu_h1_h2 = nn.ReLU()\n",
    "#         self.fc3 = nn.Linear(hidden2_size, output_size)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.fc1(x)\n",
    "#         x = self.Softmax_i_h1(x)\n",
    "#         x = self.fc2(x)\n",
    "#         x = self.relu_h1_h2(x)\n",
    "#         x = self.fc3(x)\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Activation function (HL1: ReLU)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activation function (HL1: ReLU)\n",
    "\n",
    "\n",
    "# class MLP(nn.Module):\n",
    "#     def __init__(self, input_size, hidden1_size, hidden2_size, output_size):\n",
    "#         super(MLP, self).__init__()\n",
    "#         self.fc1 = nn.Linear(input_size, hidden1_size)\n",
    "#         self.relu_i_h1 = nn.ReLU()\n",
    "#         self.fc3 = nn.Linear(hidden1_size, output_size)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.fc1(x)\n",
    "#         x = self.relu_i_h1(x)\n",
    "#         x = self.fc3(x)\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Activation function (HL1: Softmax)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Activation function (HL1: Softmax)\n",
    "\n",
    "\n",
    "# class MLP(nn.Module):\n",
    "#     def __init__(self, input_size, hidden1_size, hidden2_size, output_size):\n",
    "#         super(MLP, self).__init__()\n",
    "#         self.fc1 = nn.Linear(input_size, hidden1_size)\n",
    "#         self.softmax_i_h1 = nn.Softmax(dim=1)\n",
    "#         self.fc3 = nn.Linear(hidden1_size, output_size)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.fc1(x)\n",
    "#         x = self.softmax_i_h1(x)\n",
    "#         x = self.fc3(x)\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train MLP model with fetus training data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the MLP model\n",
    "model = MLP(input_size, hidden1_size, hidden2_size, output_size)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()  # Cross-entropy loss for classification\n",
    "optimizer = optim.SGD(\n",
    "    model.parameters(), lr=learning_rate\n",
    ")  # Stochastic Gradient Descent optimizer\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs.float())\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch [{epoch + 1}/{num_epochs}] Loss: {total_loss / len(train_loader)}\")\n",
    "\n",
    "print(\"Training completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate MLP model performance with fetus testing data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the testing data\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        outputs = model(inputs.float())\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "accuracy = 100 * correct / total\n",
    "print(f\"Accuracy on the testing data: {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot training and testing loss, training and testing accuracy across epochs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lists to store training and testing loss across epochs\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "\n",
    "# Lists to store training and testing accuracy across epochs\n",
    "train_accuracies = []\n",
    "test_accuracies = []\n",
    "\n",
    "# Training loop with recording loss and accuracy\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    correct_train = 0\n",
    "    total_train = 0\n",
    "\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs.float())\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Computing accuracy during training\n",
    "        _, predicted_train = torch.max(outputs, 1)\n",
    "        total_train += labels.size(0)\n",
    "        correct_train += (predicted_train == labels).sum().item()\n",
    "\n",
    "    # Calculate training accuracy and append to list\n",
    "    train_accuracy = 100 * correct_train / total_train\n",
    "    train_accuracies.append(train_accuracy)\n",
    "\n",
    "    # Calculate average training loss and append to list\n",
    "    train_losses.append(total_loss / len(train_loader))\n",
    "\n",
    "    # Evaluate the model on the testing data\n",
    "    model.eval()\n",
    "    correct_test = 0\n",
    "    total_test = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        test_loss = 0.0\n",
    "        for inputs, labels in test_loader:\n",
    "            outputs = model(inputs.float())\n",
    "            loss = criterion(outputs, labels)\n",
    "            test_loss += loss.item()\n",
    "\n",
    "            # Computing accuracy during testing\n",
    "            _, predicted_test = torch.max(outputs, 1)\n",
    "            total_test += labels.size(0)\n",
    "            correct_test += (predicted_test == labels).sum().item()\n",
    "\n",
    "        # Calculate testing accuracy and append to list\n",
    "        test_accuracy = 100 * correct_test / total_test\n",
    "        test_accuracies.append(test_accuracy)\n",
    "\n",
    "        # Calculate average testing loss and append to list\n",
    "        test_losses.append(test_loss / len(test_loader))\n",
    "\n",
    "\n",
    "# Plotting training and testing loss across epochs\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(train_losses, label=\"Training Loss\")\n",
    "plt.plot(test_losses, label=\"Testing Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training and Testing Loss\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plotting training and testing accuracy across epochs\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(train_accuracies, label=\"Training Accuracy\")\n",
    "plt.plot(test_accuracies, label=\"Testing Accuracy\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Training and Testing Accuracy\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aml-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
