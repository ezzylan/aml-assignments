{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Group Assignment 3 (20%)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Group Members\n",
    "\n",
    "-   Ezlan Zulfiqree bin Hashim (17192056)\n",
    "-   Nurulnadiah binti Sukerman (S2178467)\n",
    "-   Nur Dhania Kamalia Kamarol (S2121964)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Instantiate fetus training and testing datasets (Ezlan)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn import preprocessing\n",
    "\n",
    "\n",
    "# Define a custom dataset class for the fetus dataset\n",
    "class FetusDataset(Dataset):\n",
    "    def __init__(self, csv_file):\n",
    "        le = preprocessing.LabelEncoder()\n",
    "        csv_data = pd.read_csv(csv_file)\n",
    "        csv_features = csv_data.iloc[:, 1:].values\n",
    "        csv_labels = csv_data.iloc[:, 0].values\n",
    "        csv_labels = le.fit_transform(csv_labels)\n",
    "        self.features = torch.tensor(csv_features)\n",
    "        self.labels = torch.tensor(csv_labels)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.features[idx], self.labels[idx]\n",
    "\n",
    "\n",
    "# Define the paths to the training and testing CSV files\n",
    "train_csv_file = \"Fetus_trainingdata.csv\"\n",
    "test_csv_file = \"Fetus_testingdata.csv\"\n",
    "\n",
    "# Create training and testing datasets\n",
    "train_dataset = FetusDataset(train_csv_file)\n",
    "test_dataset = FetusDataset(test_csv_file)\n",
    "\n",
    "# Specify the batch size and create data loaders\n",
    "batch_size = 64\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "# You can now use train_loader and test_loader to iterate through your datasets in batches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Instantiate MLP class with init and forward methods (Ezlan)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden1_size, hidden2_size, output_size):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden1_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden1_size, hidden2_size)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# Define the input size, hidden layer sizes, and output size\n",
    "input_size = 16  # The number of features in your dataset\n",
    "hidden1_size = 10  # Number of units in the first hidden layer\n",
    "hidden2_size = 5  # Number of units in the second hidden layer\n",
    "output_size = (\n",
    "    2  # Output size for softmax (assuming binary classification, change if needed)\n",
    ")\n",
    "\n",
    "\n",
    "# Instantiate the MLP model\n",
    "model = MLP(input_size, hidden1_size, hidden2_size, output_size)\n",
    "\n",
    "# You can use this model for training and prediction tasks with your dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Train MLP model with fetus training data (Nadiah)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()  # Cross-entropy loss for classification\n",
    "optimizer = optim.SGD(\n",
    "    model.parameters(), lr=0.01\n",
    ")  # Stochastic Gradient Descent optimizer\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10  # You can adjust the number of epochs as needed\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs.float())\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch [{epoch + 1}/{num_epochs}] Loss: {total_loss / len(train_loader)}\")\n",
    "\n",
    "print(\"Training completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Evaluate MLP model performance with fetus testing data (Nadiah)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the testing data\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        outputs = model(inputs.float())\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "accuracy = 100 * correct / total\n",
    "print(f\"Accuracy on the testing data: {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Tabulate testing accuracy with different hyperparameters (Dhania)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Plot training and testing loss, training and testing accuracy across epochs (Dhania)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aml-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
